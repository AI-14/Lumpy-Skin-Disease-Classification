{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICS 485 - Machine Learning Term Project\n",
    "### Lumpy Skin Disease Classification\n",
    "### Authors: Amaan Izhar (201781130), Omar Pervez Khan (201746530)\n",
    "### Section 02, Dr. Irfan Ahmad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we applied various machine learning algorithms on an initally imbalanced dataset and recorded the results of both before and after applying oversampling and undersampling procedures. The problem at hand is a binary classification problem of predicting whether a disease is present or not. The metrics used for evaluating the performance of the models were primarily Recall and F1-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lumpy Skin Disease (LSD) is a viral infection in cattle originating in Africa which then spread to Middle East, Asia and Eastern Europe. \n",
    "The notable characteristics of this disease are fever, enlarged superficial lymph nodes, and multiple nodules on the skin. Since this infection is mainly found in cattle, its classification and detection is vital in ensuring their survival. Therefore, we utilize machine learning algorithms to detect LSD infections that were recorded in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Setup and Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardware\n",
    "- Local Machines (G14 and M1 Air)\n",
    "  \n",
    "Software / Packages\n",
    "- Jupyter\n",
    "- Sklearn\n",
    "- Matplotlib\n",
    "- Seaborn\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Imblearn\n",
    "\n",
    "Architecture / Pipeline\n",
    "1. Preprocess the data\n",
    "2. Perform data analysis\n",
    "3. Feature Selection using Chi2 method\n",
    "4. Scale the data using StandardScaler\n",
    "5. Select 5 core machine learning algorithms along with 3 ensemble algorithms\n",
    "6. Tune hyperparameters of selected algorithms\n",
    "7. Train algoritms on imbalanced dataset and record results\n",
    "8. Train algoritms on oversampled dataset and record results\n",
    "9. Train algoritms on undersampled dataset and record results\n",
    "10. Take the best model on the basis of the Recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Lumpy Skin Disease  \n",
    "Link: https://data.mendeley.com/datasets/7pyhbzb2n9/1  \n",
    "Citation: Afshari Safavi, Ehsanallah (2021), “Lumpy Skin disease dataset”, Mendeley Data, V1, doi: 10.17632/7pyhbzb2n9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explain the methodology of selection of algorithms.\n",
    "\n",
    "Step 1. We built classifiers on the whole imbalanced dataset\n",
    "The core classifiers and the reason for their selection:\n",
    "- Logistic Regression \n",
    "  - Linear classifier and the problem deals with binary classification\n",
    "- LinearSVC or Support Vector Machine\n",
    "  - Linear classifier and the problem deals with binary classification\n",
    "- Decision Tree\n",
    "  - Complex linear decision boundary to deal with an imbalanced dataset\n",
    "- Random Forest\n",
    "  - Same reason as Decision Tree but including voting as well\n",
    "- SGD (Stochastic Gradient Descent)\n",
    "  - An optimizing technique to find the optimal loss and build a decision boundary accordingly\n",
    "\n",
    "Step 2. We selected the best classifiers from Step 1 on the basis of their Recall score and applied the following ensemble methods:\n",
    "- Voting Classifier (between best 3 models from Step 1)\n",
    "  - The classifier ensembles 3 core models and average their output on a voting criteria\n",
    "  - This could enhance the performance of individual models in Step 1 taking into account the strength of each model\n",
    "- Adaboost (on the best classifier from Step 1)\n",
    "  - Boost the best model to further fine tune it\n",
    "- Bagging Classifer (on the best classifier from Step 1)\n",
    "  - Reduce variance of a black-box estimator such as Decision Tree\n",
    "\n",
    "Step 3. We repeated Step 1 and Step 2 after applying SMOTE oversampling and Random undersampling to the original dataset and recorded the results.\n",
    "\n",
    "Step 4. Once we have their performance measures - particularly recall, accuracy, and f1-score - we conduct an analysis in the section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imbalanced Dataset**  \n",
    "\n",
    "Imbalanced:\n",
    "\n",
    "Core Classifiers\n",
    "| Classifier                  \t| Recall \t| F1   \t| Accuracy \t|\n",
    "|-----------------------------\t|--------\t|------\t|----------\t|\n",
    "| Logistic Regression         \t| 0.79  \t| 0.77 \t| 0.95     \t|\n",
    "| SVM                         \t| 0.80   \t| 0.77 \t| 0.95     \t|\n",
    "| Decision Trees              \t| 0.90   \t| 0.91 \t| 0.98     \t|\n",
    "| Random Forest               \t| 0.87   \t| 0.89 \t| 0.97     \t|\n",
    "| Stochastic Gradient Descent \t| 0.82   \t| 0.78 \t| 0.95     \t|\n",
    "\n",
    "Ensemble Methods\n",
    "| Classifier          \t| Recall \t| F1   \t| Accuracy \t|\n",
    "|---------------------\t|--------\t|------\t|----------\t|\n",
    "| Voting Classifier   \t| 0.91  \t| 0.90 \t| 0.98     \t|\n",
    "| Adaboost Classifier \t| 0.90   \t| 0.91\t| 0.98     \t|\n",
    "| Bagging Classifier  \t| 0.92   \t| 0.92 \t| 0.98     \t|\n",
    "---\n",
    "\n",
    "**Oversampled Dataset**\n",
    "\n",
    "Core Classifiers\n",
    "| Classifier                  \t| Recall \t| F1   \t| Accuracy \t|\n",
    "|-----------------------------\t|--------\t|------\t|----------\t|\n",
    "| Logistic Regression         \t| 0.54   \t| 0.67 \t| 0.90     \t|\n",
    "| SVM                         \t| 0.52   \t| 0.65 \t| 0.89     \t|\n",
    "| Decision Tree               \t| 0.84   \t| 0.88 \t| 0.97     \t|\n",
    "| Random Forest               \t| 0.85   \t| 0.87 \t| 0.97     \t|\n",
    "| Stochastic Gradient Descent \t| 0.53   \t| 0.67 \t| 0.89     \t|\n",
    "\n",
    "Ensemble Methods\n",
    "| Classifier          \t| Recall \t| F1   \t| Accuracy \t|\n",
    "|---------------------\t|--------\t|------\t|----------\t|\n",
    "| Voting Classifier   \t| 0.85   \t| 0.88 \t| 0.97     \t|\n",
    "| Adaboost Classifier \t| 0.85   \t| 0.87 \t| 0.97     \t|\n",
    "| Bagging Classifier  \t| 0.84   \t| 0.88 \t| 0.97    \t|\n",
    "---\n",
    "\n",
    "**Undersampled Dataset**\n",
    "\n",
    "Core Classifiers\n",
    "| Classifier                  \t| Recall \t| F1   \t| Accuracy \t|\n",
    "|-----------------------------\t|--------\t|------\t|----------\t|\n",
    "| Logistic Regression         \t| 0.52  \t| 0.65 \t| 0.87     \t|\n",
    "| SVM                         \t| 0.50   \t| 0.64 \t| 0.87     \t|\n",
    "| Decision Trees              \t| 0.69   \t| 0.80 \t| 0.94     \t|\n",
    "| Random Forest               \t| 0.73   \t| 0.83 \t| 0.95     \t|\n",
    "| Stochastic Gradient Descent \t| 0.53   \t| 0.67 \t| 0.88     \t|\n",
    "\n",
    "Ensemble Methods\n",
    "| Classifier          \t| Recall \t| F1   \t| Accuracy \t|\n",
    "|---------------------\t|--------\t|------\t|----------\t|\n",
    "| Voting Classifier   \t| 0.73   \t| 0.83 \t| 0.95     \t|\n",
    "| Adaboost Classifier \t| 0.75   \t| 0.84 \t| 0.95     \t|\n",
    "| Bagging Classifier  \t| 0.71   \t| 0.81 \t| 0.94     \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Confusion Matrix and Classification Report are in their respective notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imbalanced Dataset**\n",
    "On an average, the error in F1 on the imbalanced dataset was calculated to be 17.6% based on individual classifiers. The best recall in the core classifiers was Decision Tree. Furthermore, the error in F1 in ensemble methods was calculated to be 9%, therefore improving significantly. The best ensemble method came out to be Bagging Classifier.\n",
    "\n",
    "**Undersampled Dataset**\n",
    "On an average, the error in F1 on the undersampled dataset was calculated to be 28.2% based on individual classifiers. The best F1 in the core classifiers was Random Forest. Furthermore, the error in F1 in ensemble methods was calculated to be 17%, therefore improving significantly. The best ensemble method came out to be Adaboost Classifier.\n",
    "\n",
    "**Oversampled Dataset**\n",
    "On an average, the error in F1 on the oversampled dataset was calculated to be 25.2% based on individual classifiers. The best F1 in the core classifiers was Random Forest as well. Furthermore, the error in F1 in ensemble methods was calculated to be 12.4%, therefore improving significantly. The best ensemble method was a tie between Adaboost Classifier and Bagging Classifier.\n",
    "\n",
    "Although undersampling dataset performs better on average error rates, our focus was on improving the recall and F1 of the individual model's error rate. Evidently from the table, there is a tie between Adaboost and Bagging for the oversampled data, so we refer to the Confusion Matrices generated to further enhance our analysis.\n",
    "\n",
    "In the confusion matrix of the Adaboost Classifier, we see that the number of correctly predicted disease (label 1) was 534 as compared to the Bagging Classifier where the number was 547. This signifies that Bagging Classifier is superior in terms of proper classification of having a disease. \n",
    "\n",
    "In conclusion, *Bagging Classifier (with a base estimator of Random Forest) on an oversampled dataset performed the best* and was able to recall more samples that had the disease. Therefore, a machine learning ecosystem can be deployed that uses this model and aids in agricultural scientists/biologists in detecting the disease and ensuring the immunity of cattle through isolating the infected ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faafacdf547457256517978ccd617e5232c831db2db3475f4239736a0901b48f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
